---
title: "Biblioteki - notatki"
output:
  html_notebook: 
    toc: true
    theme: united
  pdf_document: default
---

# viridis
Biblioteka w Pythonie *Matplotlib* wprowadziła ostatnio nowe standardy kolorów map - nazwane viridis, magma, inferno i plasma. Viridis stał się nową standardową mapą kolorów Matplotlibu. Mapy zostały tak zaprojektowane, że mogą być poprawnie postrzegane przez daltonistów, a także wydrukowane na biało-czarno.
```{r, echo=TRUE}
#install.packages("viridis")
library(viridis)
```
Żeby użyć skali viridis w ggplot używamy scale_fill_viridis() albo scale_color_viridis(). Można także jako argument w tych funkcjach podać inną skalę wyżej wymienioną, np.: scale_fill_virdis(option="magma"). Może być także używana w przypadku zmiennej dyskretnej, np.: scale_color_viridis(discrete=T)
```{r, echo=TRUE}
#install.packages("hexbin")
library(hexbin)
library(ggplot2)
ggplot(data.frame(x = rnorm(10000), y = rnorm(10000)), aes(x = x, y = y)) +
  geom_hex() + coord_fixed() +
  scale_fill_viridis() + theme_bw()
```

Funkcja viridis() generuje wektor kolorów z mapy kolorów virdis

```{r}
viridis(n=10, alpha=1, option="D")
```

option:

* "A"="magma"
* "B"="inferno"
* "C"="plasma"
* "D"="viridis"

Fukcja viridis_pal() tworzy dyskretną paletę kolorów z viridis. Poniżej fajny kawałek kodu do wizualizacji wygenerowanych kolorów

```{r}
library(scales)
show_col(viridis_pal()(10))
```
Zwróć uwagę na dziwne wywowałenie tej funkcji. 

Należy też zauważyć, że ten sam output daje viridis_pal()(10) jak i viridis(10).

# permute
Do sprawdzania, czy rozkłady, z których pochodzą próby, są takie same, stosuje się rozmaite testy statystyczne. Zawsze polegają one na obliczeniu pewnej funkcji, zwanej statystyką testową,  której  argumentem  są  zadane  próby.  Wynik  tej  funkcji  z  pomocą  tablic  (a  corazcczęściej komputera) przekształca się na p-wartość, która porównana z założonym poziomem istotności pozwala nam stwierdzić istnienie lub brak przesłanek do podważenia prawdziwości
hipotezy o równości rozkładów.

Niestety  bardzo  często  (np.  w  przypadku  testu  t-Studenta)  ich  teoretyczne  podstawy
wymagają dodatkowych założeń o porównywanych rozkładach (np. o normalności rozkładów).
Bez tych założeń nie wiedzielibyśmy jaki jest rozkład statystki testowej, a więc i p-wartości
uzyskane byłyby w sposób heurystyczny, bez dowodu, że są one poprawne. Niestety w praktyce
nie zawsze można formalnie sprawdzić czy zachodzą wymagane założenia o rozkładach.

Okazuje  się  jednak,  że  zamiast  wielu  testów  możemy  zastosować  odpowiadające  im  te-
sty permutacyjne, które nie wymagają dodatkowych założeń o rozkładach porównywanych
prób. Testy te opierają się na spostrzeżeniu, że jeżeli hipoteza zerowa o równości rozkładów
jest prawdziwa, to zamiast naszych dwóch prób mogliśmy, z takim samym prawdopodobień-
stwem, zobaczyć obserwacje o tych samych wartościach, ale innym podziałem na próby. Dla
przykładu,  gdy  naszą  obserwacją  są  dwie  dwuelementowe  próby:  ({0,2,0,37},{0,11,0,29}),
i zakładamy, że pochodzą z takich samych rozkładów, to równie prawdopodobne było zaob-
serwowanie prób ({0,2,0,29},{0,11,0,37}), czy też ({0,11,0,2},{0,29,0,37}).
Powyższe spostrzeżenie pozwala znaleźć rozkład statystki testowej, na podstawie wartości
uzyskanych przez tę funkcję przy różnych podziałach na próby. Dzięki temu możemy obliczyć
p-wartość i dalej postępować tak, jak w przypadku innych testów statystycznych.

**Randomizacja** jest to losowy dobór osób do badania. Losowość należy tutaj rozumieć, że każdy przypadek z badanej populacji ma taką samą szansę być uczestnikiem badania co każdy inny. Stosując odpowiednie metody losowania badacze "dają" szansę każdej osobie zpopulacji na bycie osobą badaną. 

Powody stosowania randomizacji w badaniach to:

1. zminimalizowanie wpływu zmiennych niekontrolowanych. Stosując losowość doboru osób do badania minimalizuje się wpływ niekontrolowanych czynników.

2. odzwierciedlenie charakterystyki populacji (np. Polacy) w badanej próbie (np. 1000 osób)

3. uprawdopodobnienie otrzymanych rezultatów badań

A permutation-based test can be used to test the same Null hypothesis, but without some of the assumptions of the t test, most importantly the assumption that the data are a random sample from the population of golden jackals. With a permutation test, we are free to choose any suitable test statistic


Biblioteka permute pozwala na dokonywanie testów permutacyjnych. Najważniejsza funkcja to shuffle(), która jest jakby wrapem funkcji sample(). Dostępny jest argument *control*, w którym definiujemy jak mają być dokonane permutacje.

```{r, results='asis'}
library(permute)
set.seed(2)
shuffle(10)
set.seed(2)
sample(1:10,10,replace=F)
```

Przykład wykorzystania funkcji shuffle() do zbadania rowności średnich w dwóch grupach na podstawie zbioru dostępnego w bibliotece permute.

```{r, results='asis'}
Djackal <- numeric(length = 5000)
data(jackal)
N <- nrow(jackal)

#funkcja do liczenia roznic srednich
meanDif <- function(x, grp) {
    mean(x[grp == "Male"]) - mean(x[grp == "Female"])
}

#dla roznych kombinacji liczy roznice srednich
set.seed(42)
for (i in seq_len(length(Djackal) - 1)) {
   perm <- shuffle(N)
   Djackal[i] <- with(jackal, meanDif(Length, Sex[perm]))
}
#ostatnim elementem jest zaobserwowana na zbiorze roznica srednich
Djackal[5000] <- with(jackal, meanDif(Length, Sex))

#ile roznic jest wiekszych niz zaobserwowana
Dbig <- sum(Djackal >= Djackal[5000])
# p-value
Dbig/length(Djackal)


```



allPerms() dokonuje wszystkich możliwych permutacji.
```{r,results='asis'}
wektor<-c(1,2,3)
allPerms(wektor)
allPerms(3)
```

numPerms() zwraca liczbę możliwych permutacji na danym obiekcie.

# foreach

Biblioteka foreach umożliwia nową konstrukcje pętli for, pozwalając na obliczenia równoległe (*parallel computing*)

operator %do% wykonuje obliczenia sekwencyjnie
operator %dopar% używa obecnie zarejestrowanego backendu (wykonuje obliczenia równolegle)

verbose(ang.) - rozwlekły

funkcja foreach() i operatory %do/%dopar są konstrukcjami pętli, które mogą być postrzegane jako hybryda pomiędzy pętlą for a funkcją lapply() - wygląda podobnie do pętli for, a dokonuje ewaluacji bardziej jak funkcja (tak jak lapply()). Ich celem jest raczej zwracanie wartości (listy defaultowo) niż powodowanie jakiś efektów ubocznych.

Obliczenia równoległe zależą od *parallel backend*, który musi być zarejestrowany przed uruchomieniem. Przykładowe biblioteki z różnymi backendami: **parallel**, **multicore**, **snow**.

Funkcja times() jest funkcją ułatwiającą użycie foreach(). Użyteczna do ewaluacji wyrażenia wielokrotnie razy kiedy nie ma zmieniających się argumentów
```{r}
library(foreach)
times(3) %do% rnorm(1)
#ekwiwalent do wywołania rnorm(3)
```

getDoParWorkers() - zwraca liczbę zarejestrowanych wątków w backendzie
getDoParRegistered() - zwraca zmienną binarną w zależności od tego czy backend został zarejestrowany

Argumenty funkcji foreach():
* .combine - do łączenia wyników. Jeśli użyjemy 'c' to wyniki zostaną połączone w wektor, 'cbind' i 'rbind' połączą wektowy w macierz. Wartości '+' i '\*' mogą być używany w celu agregacji danych numerycznych. Uważać na liczbę maksymalnych argumentów jakie przyjmuje foreach() w przypadku własnych funkcji (można zmieniać za pomocą .multicombine i .maxcobine)

Warto zwrócić uwagę na bibliotekę **iterators**.

Przykład: użycie do budowy lasu losowego
```{r}
x<-matrix(runif(500),100)
y<-gl(2,50)
library(randomForest)
rf <- foreach(ntree=rep(250, 4), .combine=combine) %do%
randomForest(x, y, ntree=ntree)
rf

#jesli bysmy chcieli odpalić żeby liczyło równolegle to najpierw musimy zarejestrować backend, a potem:
# rf <- foreach(ntree=rep(250, 4), .combine=combine, .packages='randomForest') %dopar%
# randomForest(x, y, ntree=ntree)
# rf

```
Zwróć uwagę, że żeby liczyło się równolegle to musimy w foreach() załączyć bibliotekę **randomForest**

operator %:% jest zagnieżdżonym operatorem używanym do tworzenia zagnieżdżonych pętli foreach. Operuje na dwóch obiektach foreach. Kiedy przydatne? Kiedy standardowo mamy pętle w pętli. W takim przypadki tego operatora używamy na zewnętrznej pętli. Nie jest to jednak takie oczywiste. Przykładowo jeśli zewnętrzna pętla zawiera mało iteracji może powodować problemy, a użycie na wewnętrznej pętli może nie być efektywne.
Bibliotek **doNWS** wspiera tzw *chunking tasks* - użyteczne jeśli zadania są dosyć małe i dzięki temu możemy chcieć żeby zewnętrzna pętla była uruchumiona jako pojedyńcze zadanie. chunkSize definiujemy f argumencie .options.nws funkcji foreach(). Podsumowując, używając operatora %:% możemy konwertować for pętle na zagnieżdzoną pętle foreach. Użyj %dopar% żeby obliczenia wykonywały się równolegle i ustaw opcję chunkSize tak żeby te chunki były dostatecznie duże żeby wykonywały się efektywnie, ale nie za duże tak żeby nie powodowały *load balancing problems*. Nie trzeba się koniec końców martwić o to którą pętle paralelyzować ponieważ używając operatora zmieniasz zagnieżdzone pętle w pojedyńczy stream zadań, które mogą liczone parallelnie z użyciem backendu. 



Ciekawą funkcją jest również when(), która może powstrzymać niepotrzebne ewaluacje:
```{r}
library(iterators)
x <- foreach(a=irnorm(1, count=10), .combine='c') %:% when(a >= 0) %do% sqrt(a)
x
```

# polyclip

polygon(ang.)- wielokąt

Biblioteka umożliwia dokonywanie operacji geometrycznych na wielokątach (np. suma, różnica itd.)

```{r}
library(polyclip)

A <- list(list(x=1:10, y=c(1:5,5:1)))
B <- list(list(x=c(2,8,8,2),y=c(0,0,10,10)))
plot(c(0,10),c(0,10), type="n", axes=FALSE, xlab="", ylab="")
polygon(A[[1]])
polygon(B[[1]])
C <- polyclip(A, B,op = "intersection")
polygon(C[[1]], lwd=3, col=3)
```

Kalkulacje przeprowadzane są w arytmetyce liczb całkowitych.
polylineoffset() i polyoffset() wyrysowują obszar graniczny kolejno wokół podanej linii łamanej oraz wielokątu. Granice mogą być w formie kwadratowej bądź zaokrąglonej.

polysimplify() upraszcza strukturę wielokątu.
```{r, echo=FALSE}
theta <- 2 * pi * (0:5) * 2/5
A <- list(list(x=sin(theta), y=cos(theta)))
B <- polysimplify(A, filltype="nonzero")
opa <- par(mfrow=c(1,2))
plot(c(-1,1),c(-1,1), type="n", axes=FALSE, xlab="", ylab="")
with(A[[1]], segments(x[-6], y[-6], x[-1], y[-1], col="red"))
points(A[[1]], col="red")
with(A[[1]], text(x[1:5], y[1:5], labels=1:5, cex=2))
plot(c(-1,1),c(-1,1), type="n", axes=FALSE, xlab="", ylab="")
polygon(B[[1]], lwd=3, col="green")
with(B[[1]], text(x,y,labels=seq_along(x), cex=2))
par(opa)

```



# DAAG

Biblioteka zawierająca różne zbiory danych zawartych w książce 'Data Analysis and Graphics Using R' Johna Maindonald'a oraz Johna Braun'a.


# curl




# Boruta

* redundant - zbędny
* discernible - zauważalny, dostrzegalny
* to deem - uznawać, wierzyć, sądzić
* to shuffle - przestawić, potasować
* tentative - niepewny

Pakiet służący do wyboru zmiennych objaśniających do modelu. Algorytm został obudowany wokół alorytmu lasu losowych. 
Boruta jest algorytmem selekcji wszystkich istotnych zmiennych objaśniających. Nie identyfikuje istotności zmiennej na podstawie dokładności klasyfikacji.

Trzeba pamiętać, że spadek dokładności klasyfikacji po odrzuceniu zmiennej jest wystarczający, żeby stwierdzić że jest istotna, ale brak spadku (wzrost) nie jest wystarczający żeby powiedzieć że jest nieistotna.

W Borucie istotność zmiennej jest definiowane przez porónwnanie istotności prawdziwych zmiennych do tych z przetasowanymi obserwacjami.

In a wrapper method the classifier is used as a black box returning a feature ranking.
Random forest is an ensemble method in which classification is performed by voting of multiple unbiased weak classifiers - decision trees.

**minimal optimal problem** - selection of the small (posibly minimal) feature set giving best possible classification results.

**all-relevant problem** - identification of all attributes which are in some circumstances relevant for classification. Crucial for understanding mechanisms related to the subject of interest. Ten problem rozwiązuje Boruta.

Możemy ustawić ręcznie argumenty mtry i ntree tam gdzie błąd OOB się stabilizuje ale domyślnie wyniki powinny być w porządku i bez tego.

**Argumenty:**

* doTrace=2 - wydruk będzie bardziej szczegółowy
* maxRuns - ile razy algorytm lasów losowych ma być odpalany. Jeśli po użyciu Boruty wychodzą nam zmienne tentative (niepewne), to należy zwiększyć ten parametr

**TentativeRoughFix()** - jeśli zmienne wychodzą i tak niepewne (tentative) to możemy użyć tej funkcji (albo ręcznie ocenić czy jest istotna czy nie). Porównuje mediany Z-score dla zmiennej oryginalnej i przetasowanej.

**getConfirmedFormula()**, **getNonRejectedFormula()** - wyciąga postać modelu, którą możemy użyć do klasyfikatorów

**attStats()** - Z-score dla zmiennych podczas działania kolejnych lasów losowych. Pokazuje też w ilu przypadkach wyższy niż MZSA (maximum Z-score among shadow attributes)


# caret

prevalent - dominujący, przeważający
granularity - szczegółowość, dokładność
blueprint - plan, strategia

caret - classification and regression training

predictor is called 'near zero-variance' when some category in this variable has very low frequencies.


* train() - funkcja między innymi do tunowania algorytmów machine learning.
* createDataPartition() - dokonuje podziału zbioru na treningowy i testowy. Potrzebne określenie zmiennej objaśnianej jako y, oraz podanie ile procent danych wejdzie do zbioru treningowego (argument p). Zwraca wektor wierszy wybranych do zbioru testowego.
* confusionMatrix() - tworzy tablicę pomyłek dla zbudowanego modelu.
* featurePlot() - do wizualizacji danych w postaci grafik lattice. Argument plot definiuje typ wykresu wygenerowanego.
* nearZeroVar() - funkcja do wykrywania zmiennych z prawie zerową wariancją (near zero-variance). Argument saveMetrics=T pokazuje więcej info na temat zmiennych. Kolumna freqRatio pokazuje stosunek najbardziej licznej kategorii do drugiej najbardziej licznej. Kolumna percentUnique pokazuje liczbę unikalnych wartości podzieloną przez całkowitą liczbę obserwacji razy 100. Funkcja zwróci domyślnie problematyczne zmienne.
* findCorrelation() - wyszukuje zmienne silnie skorelowane ze sobą. Argument cutoff definiuje punkt odcięcia powyżej którego zmienne zostają uznane za skorelowane ze sobą.
* findLinearCombos() - bada czy występuje problem zależności liniowej między zmiennymi.
* preProcess() - stosuje różne metody transformacji zmiennych, np. centrowanie czy skalowanie. Rodzaj transformacji definiujemy w argumencie method. Trzeba pamiętać że ta funkcja nie dokonuje od razu transformacji danych. Dopiero wtedy gdy ją odpalimy i wynik użyjemy na obciążonej funkcji predict. method="nzv" - szuka zmiennych near zero-variance.

**spatial sign transformation**
If a model is considered to be sensitive to outliers, one data transformation that can minimize the problem is the spatial sign (Serneels et al. 2006). This procedure projects the predictor values onto a multidimensional sphere. This has the effect of making all the samples the same distance from the center of the sphere. Essentially, a vector of data is divided by its norm.

```{r}
library(AppliedPredictiveModeling)
library(caret)
transparentTheme(trans = .4)

plotSubset <- data.frame(scale(mdrrDescr[, c("nC", "X4v")])) 
xyplot(nC ~ X4v,
       data = plotSubset,
       groups = mdrrClass, 
       auto.key = list(columns = 2))  

```

```{r}
# po transformacji "spatial sign"
transformed <- spatialSign(plotSubset)
transformed <- as.data.frame(transformed)
xyplot(nC ~ X4v, 
       data = transformed, 
       groups = mdrrClass, 
       auto.key = list(columns = 2)) 

```




ROZKMINIĆ JAK DZIAŁA CLASS DISTANCE CALCULATIONS, ODLEGLOSC MAHALANOBISA I LDA




# dplyr

remainder - pozostałość, reszta
coarse - toporny, szorstki


* tbl_df() - wrapper wokół ramki danych, przydatny w pracy z dużymi zbiorami danych. Nie wydrukuje przypadkowo danych do konsoli.
* filter() - wybiera subset wierszy z ramki. Pierwszym argumentem ramka, nastepnymi wyrażenia filtrujące.
* slice() - tnie ramkę po wybranych numerach wierszy.
* arrange() - order by na ramce. Można definiować wiele kolumn, a jak chcemy malejąco to stosujemy desc().
* select() - wybiera subset kolumn z ramki. Można stosować wyrażenia takie jak: kol1:kol7 albo -(kol1:kol7). Także warto stosować takie funkcje w argumentach jak: starts_with(), ends_with(), matches(), contains().
* rename() - zmienia nazwę kolumny.
* distinct() - do wyszukiwania unikalnych wartości w kolumnie/kolumnach. Podobne do unique() ale powinno być szybsze.
* mutate() - dodaje nowe kolumny jako przekształcenia już istniejących w tabeli. Działa podobnie do transform() z base'a i tak samo do funkcji z biblioteki plyr. 
* transmute() - Jeśli chcemy zostawić tylko nowe zmienne, które przekształciliśmy a stare wyrzucić.
* summarise() -  Tworzy podsumowania jakiś kolumn. Takie samo działanie jak plyr::summarise().
* sample_n() - losowanie n losowych wierszy z tabeli. N jest liczbą. Argument replace=T żeby stworzyć bootstrapową próbę.
* sample_frac() - losowanie n losowych wierszy z tabeli. N jest ułamkiem. Argument replace=T żeby stworzyć bootstrapową próbę.

```{r, include=FALSE}
#zad 1
library(dplyr)
df=data.frame(Theoph)
```

```{r}
#zad 2
names(df)
```

```{r}
#zad 3
select(df,Subject:Dose)
# select(df,Wt, Dose)
```

```{r}
#zad 4
filter(df, Dose>5)
```

```{r}
#zad 5
filter(df, Dose>5, Time>mean(Time))
# filter(df, Dose>5 & Time>mean(Time))
```

```{r}
#zad 6
# arrange(df,desc(Wt))
# arrange(df,Wt)
arrange(df,Wt,desc(Time))
```

```{r}
#zad 7
mutate(df, trend = Time-mean(Time,na.rm=T))
```

```{r}
#zad 8
df<-mutate(df, weight_cat = ifelse(Wt < 69.85 , "Welterweight",ifelse(Wt < 72.57 , "Light-middleweight",ifelse(Wt < 76.2  , "middleweight", ifelse(Wt>76.2, "Super-Middleweight", "Unknown")))))
```

```{r}
#zad 9
weight_group <- group_by(df, weight_cat)
```

```{r}
#zad 10
summarise(weight_group,
          meanTime = mean(Time, na.rm = T),
          sumDose = sum(Dose, na.rm = T))
```

## Funkcje okienkowe


```{r}
library(Lahman)
batting <- select(tbl_df(Batting), playerID, yearID, teamID, G, AB:H) 
batting <- arrange(batting, playerID, yearID, teamID)
players <- group_by(batting, playerID)
```

```{r}
# For each player, find the two years with most hits
filter(players, min_rank(desc(H)) <= 2 & H > 0)
```

```{r}
# Within each player, rank each year by the number of games played
mutate(players, G_rank = min_rank(G))
```

```{r}
# For each player, find every year that was better than the previous year
filter(players, G > lag(G))
```

```{r}
# For each player, compute avg change in games played per year
mutate(players, G_change = (G - lag(G)) / (yearID - lag(yearID)))
```

```{r}
# For each player, find all where they played more games than average
filter(players, G > mean(G))
```

```{r}
# For each, player compute a z score based on number of games played
mutate(players, G_z = (G - mean(G)) / sd(G))
```

```{r}
#pokaz tylko 2 najlepsze lata pod wzgledem ilosci wystepow
filter(players, min_rank(desc(G)) <= 2)
```

```{r}
#ile srednio wystepowali pilkarze w podziale na 4 grupy
by_team_player <- group_by(batting, teamID, playerID)
by_team <- summarise(by_team_player, G = sum(G))
by_team_quartile <- group_by(by_team, quartile = ntile(G, 4))
summarise(by_team_quartile, mean(G))
```

```{r}
#roznica miedzy kolejnymi rozegranymi meczami
mutate(players, G_delta = G - lag(G))
```

```{r}
#tylko zawodnicy ktorzy zmienili team
filter(players, teamID != lag(teamID))
```

```{r}
#pamietac przy funkcjach agregujacych o odpowiedniej kolejnosci wierszy(order_by)
df <- data.frame(year = 2000:2005, value = (0:5) ^ 2)
scrambled <- df[sample(nrow(df)), ]

wrong <- mutate(scrambled, running = cumsum(value))
arrange(wrong, year)

right <- mutate(scrambled, running = order_by(year, cumsum(value)))
arrange(right, year)
```

```{r}
#zawodnicy ktorzy chociaz w jednym sezonie mieli powyzej 150 wystepow
filter(players, cumany(G > 150))
```

```{r}
#sezony zawodnikow z meczami powyzej 150
filter(players, cumall(G > 150))
```

```{r}
#ile lat w klubie wiersz po wierszu
mutate(players, career_year = yearID - min(yearID) + 1)
```



Nie czytałem o strukturze funkcji okienkowych w SQL !!!



# tidyr

Biblioteka pomagająca w ogarnięciu nieuporządkowanych zbiorów danych (messy datasets). Często używana razem z biblioteką dplyr.

Jakie wyróżniamy problemy w takich zbiorach danych?
* Column headers are values, not variable names.
* Multiple variables are stored in one column.
* Variables are stored in both rows and columns.
* Multiple types of observational units are stored in the same table.
* A single observational unit is stored in multiple tables.

implicit - domyślny, bezwarunkowy, ukryty
explicit - jednoznaczny, jawny
gather - zbierać, zebrać, zgarnąć
spread - rozprzestrzeniać, rozprowadzać

```{r}
library(tidyr)
library(dplyr)
head(mtcars)
```

```{r}
mtcars$car <- rownames(mtcars)
mtcars <- mtcars[, c(12, 1:11)]
rownames(mtcars) <- 1:nrow(mtcars)
```

```{r}
#zbiera wszystkie nazw zmiennych do zmiennej attribute i odpowiadajace im wartosci do value. Zostawia tylko car,
#ktory bedzie replikowany
mtcars %>%
  gather(., attribute, value, -car) -> mtcarsNew
head(mtcarsNew)
```

```{r}
mtcars %>% 
  gather(., attribute, value, mpg:gear) -> mtcarsNew
head(mtcarsNew)
```

```{r}
#za pomoca spread mozemy wrocic do oryginalnego ksztaltu
mtcarsNew %>%
  spread(., attribute, value)
```

```{r}
set.seed(1)
date <- as.Date('2016-01-01') + 0:14
hour <- sample(1:24, 15)
min <- sample(1:60, 15)
second <- sample(1:60, 15)
event <- sample(letters, 15)
data <- data.frame(date, hour, min, second, event)
data
```

```{r}
#laczy kolumy date, hour, min i second w datetime
dataNew <- data %>%
  unite(., datehour, date, hour, sep = ' ') %>%
  unite(., datetime, datehour, min, second, sep = ':')
dataNew
```

```{r}
#za pomoca separate wracamy do oryginalnego ksztaltu
data1 <- dataNew %>% 
  separate(datetime, c('date', 'time'), sep = ' ') %>% 
  separate(time, c('hour', 'min', 'second'), sep = ':')
data1
```


Funkcje:
* gather() - konwertuje 'szeroką' ramkę danych na dłuższy format. Analogiczna do funkcji melt() z reshape2
* spread() - odwrotnie do gather(). Analogiczna do funkcji cast() z reshape2
* unite() - tworzy pojedynczą kolumnę z dwóch lub więcej
* seperate() - dzieli jedną kolumnę na dwie lub więcej


# tibble

Objekt tbl_df  jest ramką danych, która posiada lepszą metodę jej wyświetlania (szczególnie użyteczna gdy pracujemy z dużymi zbiorami danych). Klasa S3 tbl_df wrapuje lokalną ramkę danych. Głowną zaletą tibbles jest to, że drukują tylko pierwsze 10 wierszy i wszystkie kolumny, które mieszczą się na ekranie. Ponadto ewaluują swoje argumenty leniwie (lazy evaluation), np.:

```{r}
tibble(x = 1:5, y = x ^ 2)
```
Podczas gdy w ramce danych to nie zadziała:
```{r}
data.frame(x = 1:5, y = x ^ 2)
```

Tibbles nie używa row.names(), ponieważ chce utrzymywać dane w spójny sposób.

Ponadto bardziej efektywne jest przekształcenie jakiś danych (np. listy) do tibble niż do data.frame:
```{r}
l <- replicate(26, sample(100), simplify = FALSE)
names(l) <- letters

microbenchmark::microbenchmark(
  as_tibble(l),
  as.data.frame(l)
)
```
as_tibble() działa to na zasadzie zastosowania do.call(cbind, lapply(x, data.frame))

Różnice między tibbles a data frames:

* drukowanie - drukując tibble wyświetlane jest tylko 10 pierwszych wierszy i wszystkie kolumny, które mieszczą się na ekranie. Można kontrolować te opcje poprzez: options(tibble.print_max = n, tibble.print_min = m). options(tibble.width = Inf) pokaże wszystkie kolumny, niezależnie od szerokości ekranu
* subsetting - używając operatora '[' tibble zawsze zwraca tibble, w przeciwieństwie do ramki danych, która czasami może zwrócić wektor. Żeby wybrać pojedynczą kolumnę z tiblle należy użyć operatora '[[' albo \$. Także jeśli za pomocą operatora \$ wybieramy kolumnę, której tam nie ma to otrzymamy NULL, podczas gdy wykonując to na tibble otrzymamy dodatkowo ostrzeżenie.
* tibble nie konwertuje też nigdy zmiennej string na factor, nie zmienia nazw zmiennych i nie tworzy nazw wierszy!


Funkcje:

* as_tibble() (alias as_data_frame()) - konwertuje ramkę danych do tibble (tbl_df). Jak wskazana powyżej as_tibble() jest dużo bardziej efektywne niż as.data.frame()
* tibble() (alias data_frame) - tworzy tibble
* add_column() - dodaje kolumnę do ramki danych (tibble)
* add_row() - dodaje wiersz do ramki danych
* enframe() - konwertuje wektor z nazwami albo listy na dwu-kolumnowe ramki danych.
```{r}
enframe(4:7)
enframe(c(x=1, y=2, z=3))
```
* glimpse() - transponowana wersja print. Kolumny są wyświetlane jako wiersze. Działa podobnie jak funkcja str() użyta na ramce danych, ale stara się wyświetlić tak wiele danych jak możliwe.
```{r}
glimpse(mtcars)
```
* has_name(x, name) - sprawdza czy dana kolumna występuje w ramce danych
* lst() - podobna funkcja do list(), z tym że tak jak tibble ewaluacja jest leniwa i automatycznie dodaje nazwy.
```{r}
lst(n=5, x=rnorm(n))
```
Podczas gdy:
```{r}
list(n=5, x=rnorm(n))
```

# microbenchmark

Biblioteka do porównywania czasu wykonania różnych wyrażeń R.

Funkcje:

* autoplot() - rysuje z wykorzystaniem ggplot2 czasy wykonań na podstawie obiektu microbenchmark
* boxplot() - rysuje boxplot na obiekcie microbenchmark
* microbenchmark() - funkcja do porównywania czasów wykonania. Argumenty: times - ile razy wykonać dane wyrażenie, unit - w jakich jednostkach czasu drukować czasy wykonywania ("ms", "s" ..., domyślnie nanosekundy), control - order: kolejność wykonywania wyrażeń (domyślnie losowo "random", można też "inorder", wtedy wykonuje po kolei, albo "block" - wtedy wykonuje jako jeden blok), warmup: liczba startowych iteracji zanim będzie mierzony faktycznie czas wykonania (domyślnie ustawiony na 2)

```{r}
library(microbenchmark)
library(ggplot2)

tm <- microbenchmark(rchisq(100, 0),
    rchisq(100, 1),
    rchisq(100, 2),
    rchisq(100, 3),
    rchisq(100, 5), times=1000L)
```

```{r}
print(tm)
```

```{r}
summary(tm)
```
```{r}
autoplot(tm)
```

```{r}
boxplot(tm)
```




